{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMKz19Y7N2SoFX/mRJUY7w", 
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedwillTech/MedwillTech/blob/main/NotionEngeneering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap sur Ratissage du Web"
      ],
      "metadata": {
        "id": "xCbbcr4HUwVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse syntaxique duHTM"
      ],
      "metadata": {
        "id": "69V0ozJSU2eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m pip install beautifulsoup4 requests html5lib\n",
        "\"\"\"\n",
        " <html>\n",
        "  <head>\n",
        "    <title>A web page</title>\n",
        "  </head>\n",
        "  <body>\n",
        "    <p id=\"author\">Joel Grus</p>\n",
        "    <p id=\"subject\">Data Science</p>\n",
        "  </body>\n",
        " </html>\n",
        "\n",
        "\"\"\"\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        " # Je place le fichier HTML qui nous occupe sur GitHub. Pour adapter\n",
        " # l’URL au format de l’ouvrage, je dois la répartir sur deux lignes.\n",
        " # Rappelez-vous que les chaînes séparées par des caractères d’espacement\n",
        " # sont concaténées.\n",
        "url = (\"https://raw.githubusercontent.com/\"\n",
        "       \"joelgrus/data/master/getting-data.html\")\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "print(soup.contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IavmRmXEg9Yk",
        "outputId": "95aee920-95b6-4166-f28f-18cccad03f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['html', '\\n', <html lang=\"en-US\">\n",
            "<head>\n",
            "<title>Getting Data</title>\n",
            "<meta charset=\"utf-8\"/>\n",
            "</head>\n",
            "<body>\n",
            "<h1>Getting Data</h1>\n",
            "<div class=\"explanation\">\n",
            "        This is an explanation.\n",
            "    </div>\n",
            "<div class=\"comment\">\n",
            "        This is a comment.\n",
            "    </div>\n",
            "<div class=\"content\">\n",
            "<p id=\"p1\">This is the first paragraph.</p>\n",
            "<p class=\"important\">This is the second paragraph.</p>\n",
            "</div>\n",
            "<div class=\"signature\">\n",
            "<span id=\"name\">Joel</span>\n",
            "<span id=\"twitter\">@joelgrus</span>\n",
            "<span id=\"email\">joelgrus-at-gmail</span>\n",
            "</div>\n",
            "</body>\n",
            "</html>, '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.p.text.split())\n",
        "print(soup.find_all(\"p\"))\n",
        "\n",
        "important_paragraphs = soup('p', {'class' : 'important'})\n",
        "important_paragraphs2 = soup('p', 'important')\n",
        "important_paragraphs3 = [p for p in soup('p')\n",
        "                         if 'important' in p.get('class', [])]\n",
        "\n",
        "print(important_paragraphs)\n",
        "print(important_paragraphs2)\n",
        "print(important_paragraphs3)\n",
        "\n",
        "# Par exemple, si vous cherchez tous les éléments <span> contenus dans un élément\n",
        "# <div>, procédez commesuit:\n",
        "\n",
        " # Attention, retourne le même <span> plusieurs fois de suite\n",
        " # s'il est présent dans plusieurs <div>.\n",
        " # soyez plus malin dans ce cas\n",
        "spans_inside_divs = [span\n",
        "                 # pour chaque <div> sur la page\n",
        "                 for div in soup('div')\n",
        "                 # trouver chaque <span> à l’intérieur\n",
        "                 for span in div('span')]\n",
        "\n",
        "\n",
        "print(spans_inside_divs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blTMPMJvkaZa",
        "outputId": "29c60bf8-22b2-41a3-c1bf-b4ed9644238d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'paragraph.']\n",
            "[<p id=\"p1\">This is the first paragraph.</p>, <p class=\"important\">This is the second paragraph.</p>]\n",
            "[<p class=\"important\">This is the second paragraph.</p>]\n",
            "[<p class=\"important\">This is the second paragraph.</p>]\n",
            "[<p class=\"important\">This is the second paragraph.</p>]\n",
            "[<span id=\"name\">Joel</span>, <span id=\"twitter\">@joelgrus</span>, <span id=\"email\">joelgrus-at-gmail</span>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exemple: Congrès sous surveillance**"
      ],
      "metadata": {
        "id": "k1RyUDGsVFUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le vice-président chargé des politiques chez DataSciencester s’inquiète d’une potentielle réglementation du secteur de la science des données. Il vous demande de quantifier ce que le Congrès a à dire sur le sujet. Il veut plus particulièrement que vous identifiiez tous les repré sentants de cette institution des États-Unis qui sont associés à des communiqués de presse\n",
        "relatifs aux données.\n",
        "Au moment où nous publions ces lignes, une page contient des liens vers tous les sites web des représentants: https://www.house.gov/representatives.\n",
        "Et si vous consultez la source, vous constatez que tous les liens vers ces sites web se présentent selon l’exemple suivant:"
      ],
      "metadata": {
        "id": "AdsoFqs0VICq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# URL de la page des représentants\n",
        "url = \"https://www.house.gov/representatives\"\n",
        "text = requests.get(url).text\n",
        "\n",
        "# Utilisation du parseur HTML standard de Python\n",
        "soup = BeautifulSoup(text, \"html.parser\")\n",
        "\n",
        "# Trouver tous les liens <a> qui contiennent les informations sur les représentants\n",
        "# Nous allons chercher des liens qui sont typiquement dans des balises <a> de la liste des représentants\n",
        "all_urls = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "\n",
        "# Affichage du nombre total de liens\n",
        "print(f\"Total des URLs trouvées : {len(all_urls)}\")\n",
        "\n",
        "# Expression régulière pour filtrer les liens .house.gov\n",
        "regex = r\"^https?://.*\\.house\\.gov/?$\"\n",
        "good_urls = [url for url in all_urls if re.match(regex, url)]\n",
        "\n",
        "# Suppression des doublons en utilisant un set\n",
        "good_urls = list(set(good_urls))\n",
        "\n",
        "# Affichage du nombre de bonnes URLs\n",
        "print(f\"Nombre d'URLs filtrées (représentants) : {len(good_urls)}\")\n",
        "\n",
        "# Extraction des noms des représentants à partir des bonnes URLs\n",
        "representatives = []\n",
        "for house_url in good_urls:\n",
        "    html = requests.get(house_url).text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Recherche des noms des représentants dans des éléments <h1> ou autres balises pertinentes\n",
        "    name = soup.find('h1')  # Habituellement, les noms des représentants sont dans <h1> ou <title>\n",
        "    if name:\n",
        "        representatives.append(name.get_text(strip=True))\n",
        "\n",
        "# Affichage des 20 premiers noms de représentants\n",
        "print(\"Les 20 premiers représentants trouvés :\")\n",
        "for i, rep in enumerate(representatives[:20]):\n",
        "    print(f\"{i + 1}. {rep}\")\n",
        "\n",
        "# Si vous souhaitez plus de détails sur chaque représentant (exemple : leurs liens de presse)\n",
        "press_releases = {}\n",
        "for house_url in good_urls[:20]:  # Limiter aux 20 premiers pour éviter trop de requêtes\n",
        "    html = requests.get(house_url).text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Recherche des liens de presse sur la page du représentant\n",
        "    pr_links = {a['href'] for a in soup.find_all('a', href=True) if 'press releases' in a.text.lower()}\n",
        "    press_releases[house_url] = pr_links\n",
        "\n",
        "# Affichage des liens de presse pour les 20 premiers\n",
        "for house_url, pr_links in press_releases.items():\n",
        "    print(f\"\\nLiens de presse pour {house_url}:\")\n",
        "    for pr_link in pr_links:\n",
        "        print(f\" - {pr_link}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHHnyRO2rV1y",
        "outputId": "a2f06790-2a49-47f3-c543-09ce3d6cda0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total des URLs trouvées : 967\n",
            "Nombre d'URLs filtrées (représentants) : 439\n",
            "Les 20 premiers représentants trouvés :\n",
            "1. Home\n",
            "2. ICYMI: Stefanik Highlights House Republicans' Oversight Uprooting Corruption: \"It's Our Job to Get Transparency and Accountability\"\n",
            "3. Home\n",
            "4. Home\n",
            "5. U.S. Congressman French Hill. Serving the 2nd District of Arkansas.\n",
            "6. Home\n",
            "7. Proudly Serving theVirgin Islands of the United States\n",
            "8. Home\n",
            "9. Home\n",
            "10. Home\n",
            "11. Home\n",
            "12. How Can I Help?\n",
            "13. PROUDLYSERVING FLORIDA\n",
            "14. REPORT & VIDEO: Congresswoman Ross Releases 2024 Year-End Report\n",
            "15. Home\n",
            "16. Let's Stay Connected\n",
            "17. David Scott. Representing Georgia's 13th District\n",
            "18. Home\n",
            "19. Representing New York's 26th District\n",
            "20. Hi, I'mRep. Gluesenkamp Perez\n",
            "\n",
            "Liens de presse pour https://lofgren.house.gov/:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://ansari.house.gov/:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://mrvan.house.gov:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://stefanik.house.gov/:\n",
            " - /press-releases\n",
            "\n",
            "Liens de presse pour https://susielee.house.gov:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://rulli.house.gov:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://timmoore.house.gov/:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://hill.house.gov/:\n",
            " - /news/documentquery.aspx?DocumentTypeID=27\n",
            "\n",
            "Liens de presse pour https://barrett.house.gov/:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://plaskett.house.gov/:\n",
            " - /news/documentquery.aspx?documenttypeid=27\n",
            "\n",
            "Liens de presse pour https://mcclellan.house.gov/:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://panetta.house.gov:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://wied.house.gov/:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://mcdonaldrivet.house.gov/:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://craig.house.gov:\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://neal.house.gov/:\n",
            " - /news/documentquery.aspx?DocumentTypeID=27\n",
            "\n",
            "Liens de presse pour https://waltz.house.gov:\n",
            " - /news/documentquery.aspx?DocumentTypeID=27\n",
            "\n",
            "Liens de presse pour https://ross.house.gov:\n",
            " - /press-releases\n",
            "\n",
            "Liens de presse pour https://dondavis.house.gov:\n",
            " - /issues/grants\n",
            " - /media/press-releases\n",
            "\n",
            "Liens de presse pour https://balderson.house.gov:\n",
            " - /news/documentquery.aspx?DocumentTypeID=27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recap sur les Fichiers à délimiteur**"
      ],
      "metadata": {
        "id": "wDkX6aWNVr1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exemple de datei csv ou txt\n",
        "# Nom du fichier est: tab_delimited_stock_prices.txt\n",
        "\n",
        "6/20/2014 AAPL 90.91\n",
        "6/20/2014 MSFT 41.68\n",
        "6/20/2014 FB 64.5\n",
        "6/19/2014 AAPL 91.86\n",
        "6/19/2014 MSFT 41.51\n",
        "6/19/2014 FB 64.34\n",
        "\n",
        "import csv\n",
        "with open('tab_delimited_stock_prices.txt') as f:\n",
        "  tab_reader = csv.reader(f, delimiter='\\t') # on peut avoir aussi  delimiter=':' ou '-' ou ',' etc...\n",
        "  for row in tab_reader:\n",
        "    date = row[0]\n",
        "    symbol = row[1]\n",
        "    closing_price = float(row[2])\n",
        "    # traitement des données...\n"
      ],
      "metadata": {
        "id": "owAPffQSe2Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recap sur la Lecture de fichiers**"
      ],
      "metadata": {
        "id": "mz4oqeUoV20-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_domain(email_address: str) -> str:\n",
        "  \"\"\"couper au niveau de '@' et retourner le dernier morceau\"\"\"\n",
        "  return email_address.lower().split(\"@\")[-1]\n",
        "\n",
        " # deux tests\n",
        "assert get_domain('joelgrus@gmail.com') == 'gmail.com'\n",
        "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'\n",
        "\n",
        "from collections import Counter\n",
        "with open('email_addresses.txt', 'r') as f:\n",
        "  domain_co unts = Counter(get_domain(line.strip())\n",
        "                           for line in f\n",
        "                           if \"@\" in line)"
      ],
      "metadata": {
        "id": "TMBPejJ_d86A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recap sur les: \"stdin\" et \"stdout\"**"
      ],
      "metadata": {
        "id": "f41ex9OuWCGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# en Bash: cat output.txt | python test.py\n",
        "# en Windows: type output.txt | python test.py\n",
        "# otput.txt--> contient le text\n",
        "# test.üy---> contient le bout de code python\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        " # passer le nombre de mots en premier argument\n",
        "try:\n",
        "    num_words = int(sys.argv[1])\n",
        "except:\n",
        "    print(\"usage: test.py num_words\")\n",
        "    sys.exit(1)  # un code non zéro en sortie indique une erreur\n",
        "counter = Counter(word.lower()  # mots en minuscules\n",
        "                  for line in sys.stdin\n",
        "                  # séparer à partir des espaces\n",
        "                  for word in line.strip().split()\n",
        "                  if word)  # sauter les mots (words) vides\n",
        "for word, count in counter.most_common(num_words):\n",
        "  sys.stdout.write(str(count))\n",
        "  sys.stdout.write(\"\\t\")\n",
        "  sys.stdout.write(word)\n",
        "  sys.stdout.write(\"\\n\")"
      ],
      "metadata": {
        "id": "C7-QEDDoYRsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
